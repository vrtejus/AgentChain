{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyO3LkwSCrYsd/sKW9UrajA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrtejus/AgentChain/blob/main/Nooks_ML_Takehome_Solved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install torchsampler\n",
        "!pip3 install nlpaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFLVzZ6igWTR",
        "outputId": "1bf983c1-169d-4748-fd2f-44cf6a331596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: torchsampler in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from torchsampler) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from torchsampler) (0.16.0+cu118)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torchsampler) (1.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->torchsampler) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->torchsampler) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->torchsampler) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->torchsampler) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->torchsampler) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->torchsampler) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->torchsampler) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5->torchsampler) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5->torchsampler) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5->torchsampler) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsampler) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsampler) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->torchsampler) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->torchsampler) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->torchsampler) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->torchsampler) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->torchsampler) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->torchsampler) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->torchsampler) (1.3.0)\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua1gbcYFeJXd",
        "outputId": "0330a847-49c4-49d6-e801-79b498cbb47a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/takehome_data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from torchsampler import ImbalancedDatasetSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "import nlpaug.augmenter.word as naw"
      ],
      "metadata": {
        "id": "kB4B-g-2fhPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "LVD2i8CrnBzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(f\"{BASE_PATH}/call_data.csv\")\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "A-_Lt5NYfRfT",
        "outputId": "f2e1920c-2b69-4c5f-e842-4558faf02dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  SID  Had Timing Objection  \\\n",
              "0  CAb91afa254ef65b0d75c24e16edd9c4b7                 False   \n",
              "1  CA3b8e6bbc08808390ddc4c8c36fb8ba5f                 False   \n",
              "2  CA43cdd0c490f4e49474dda60800caba77                  True   \n",
              "3  CA535b77a335e9d2cd49c0158215b96133                 False   \n",
              "4  CAfd5dd1a208651485bc5bc0b85f0555da                 False   \n",
              "\n",
              "  Timing Objection Index  \n",
              "0                   None  \n",
              "1                   None  \n",
              "2                     43  \n",
              "3                   None  \n",
              "4                   None  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7faaf302-0b7f-4dd9-9993-bbe38251a2e2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>Had Timing Objection</th>\n",
              "      <th>Timing Objection Index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CAb91afa254ef65b0d75c24e16edd9c4b7</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CA3b8e6bbc08808390ddc4c8c36fb8ba5f</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CA43cdd0c490f4e49474dda60800caba77</td>\n",
              "      <td>True</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CA535b77a335e9d2cd49c0158215b96133</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CAfd5dd1a208651485bc5bc0b85f0555da</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7faaf302-0b7f-4dd9-9993-bbe38251a2e2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7faaf302-0b7f-4dd9-9993-bbe38251a2e2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7faaf302-0b7f-4dd9-9993-bbe38251a2e2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3e0ce545-7d2c-4520-8c21-6fb2496abdf8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3e0ce545-7d2c-4520-8c21-6fb2496abdf8')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3e0ce545-7d2c-4520-8c21-6fb2496abdf8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_folder = f\"{BASE_PATH}/transcripts\"\n",
        "df['Transcript'] = None\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    sid = row['SID']\n",
        "    transcript_path = os.path.join(transcript_folder, f\"{sid}.txt\")\n",
        "    if os.path.exists(transcript_path):\n",
        "        with open(transcript_path, 'r') as f:\n",
        "            transcript = f.read()\n",
        "        df.at[idx, 'Transcript'] = transcript\n",
        "    else:\n",
        "        print(f\"Transcript not found for SID: {sid}\")\n",
        "\n",
        "# Check the first few rows to ensure transcripts are loaded\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kThyjE0f0tM",
        "outputId": "cc66b1c9-6919-41b0-c04c-7af96607e63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  SID  Had Timing Objection  \\\n",
            "0  CAb91afa254ef65b0d75c24e16edd9c4b7                 False   \n",
            "1  CA3b8e6bbc08808390ddc4c8c36fb8ba5f                 False   \n",
            "2  CA43cdd0c490f4e49474dda60800caba77                  True   \n",
            "3  CA535b77a335e9d2cd49c0158215b96133                 False   \n",
            "4  CAfd5dd1a208651485bc5bc0b85f0555da                 False   \n",
            "\n",
            "  Timing Objection Index                                         Transcript  \n",
            "0                   None  1. [Sales Rep] Hello?\\n2. [Prospect] Hey, Jane...  \n",
            "1                   None  1. [Sales Rep] Hello?\\n2. [Prospect] Hey, Eric...  \n",
            "2                     43  1. [Sales Rep] Hi. It's Alan.\\n2. [Prospect] A...  \n",
            "3                   None  1. [Prospect] Hello?\\n2. [Sales Rep] Yeah.\\n3....  \n",
            "4                   None  1. [Sales Rep] Hello?\\n2. [Prospect] Peter, th...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and tokenize the text data\n",
        "df['tokenized_text'] = df['Transcript'].apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=512, return_tensors='pt') if pd.notnull(x) else None)\n"
      ],
      "metadata": {
        "id": "AMPijbFogM8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n",
        "\n",
        "# Verify if a GPU is available and if so, use it\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGtNKcUsgNRA",
        "outputId": "57a9c0ca-8296-4101-9c4c-703206ac1dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting"
      ],
      "metadata": {
        "id": "nY56HgaLgqTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where transcripts are missing\n",
        "df = df.dropna(subset=['Transcript'])\n",
        "\n",
        "# Split the data\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "2SQz0Mu-gQ9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EpvNPmMr3NS",
        "outputId": "53495728-11ad-4d92-a182-da75cbcd4147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    SID  Had Timing Objection  \\\n",
            "79   CA714dba08054505f4063d5378f173ffea                 False   \n",
            "197  CAe2e02e272554e568be0ce0fbd9e8a7cd                 False   \n",
            "38   CA56b95700fb1660ae63b5f58b26efed9d                 False   \n",
            "24   CA8fa65fe9b916f14b0985ed52afd96175                 False   \n",
            "122  CAf51ce847e20f4bbac4dfd2468929b704                 False   \n",
            "\n",
            "    Timing Objection Index                                         Transcript  \\\n",
            "79                    None  1. [Sales Rep] Hello?\\n2. [Prospect] Hey. Is t...   \n",
            "197                      1  1. [Sales Rep] Please leave your message for e...   \n",
            "38                    None  1. [Prospect] Hi. Is this Patrick? Hey. This i...   \n",
            "24                    None  1. [Sales Rep] This is Jennifer. Okay. Okay.\\n...   \n",
            "122                      1                     1. [Sales Rep] Hello? Hello?\\n   \n",
            "\n",
            "                  tokenized_text  \n",
            "79   [input_ids, attention_mask]  \n",
            "197  [input_ids, attention_mask]  \n",
            "38   [input_ids, attention_mask]  \n",
            "24   [input_ids, attention_mask]  \n",
            "122  [input_ids, attention_mask]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the Minority Class\n",
        "minority_class = train_df['Had Timing Objection'].value_counts().idxmin()\n",
        "\n",
        "# Calculate the Oversampling Amount\n",
        "minority_count = train_df[train_df['Had Timing Objection'] == minority_class].shape[0]\n",
        "majority_count = train_df[train_df['Had Timing Objection'] != minority_class].shape[0]\n",
        "\n",
        "samples_to_generate = majority_count - minority_count\n",
        "\n",
        "# Oversample the Minority Class (duplicate samples)\n",
        "minority_samples = train_df[train_df['Had Timing Objection'] == minority_class]\n",
        "\n",
        "# Use Text Data Augmentation\n",
        "text_augmenter = naw.SynonymAug(aug_src='wordnet')\n",
        "oversampled_texts = []\n",
        "for text in minority_samples['Transcript'].values:\n",
        "    augmented_text = text_augmenter.augment(text)\n",
        "    if isinstance(augmented_text, str):\n",
        "        oversampled_texts.append(augmented_text)\n",
        "    elif isinstance(augmented_text, list):\n",
        "        oversampled_texts.append(' '.join(augmented_text))\n",
        "    else:\n",
        "        raise TypeError(\"Unexpected output type from text augmenter\")\n",
        "\n",
        "# Make sure we have generated enough samples\n",
        "if len(oversampled_texts) < samples_to_generate:\n",
        "    deficit = samples_to_generate - len(oversampled_texts)\n",
        "    additional_samples = minority_samples.sample(deficit, replace=True)['Transcript'].tolist()\n",
        "    oversampled_texts.extend(additional_samples)\n",
        "\n",
        "oversampled_labels = [minority_class] * samples_to_generate\n",
        "oversampled_df = pd.DataFrame({'Transcript': oversampled_texts, 'Had Timing Objection': oversampled_labels})\n",
        "train_df_oversampled = pd.concat([train_df, oversampled_df]).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "Cn6h-wyYrK3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with oversampled dataset"
      ],
      "metadata": {
        "id": "y9Z6OGbosif_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data\n",
        "tokenized_texts = tokenizer(list(train_df_oversampled['Transcript']), truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "train_input_ids = tokenized_texts['input_ids']\n",
        "train_attention_mask = tokenized_texts['attention_mask']\n",
        "\n",
        "# Ensure that labels are a tensor and have the correct shape\n",
        "train_labels = torch.tensor(train_df_oversampled['Had Timing Objection'].values).unsqueeze(1).float()\n",
        "\n",
        "# Check that all tensors have the same number of rows\n",
        "assert train_input_ids.shape[0] == train_attention_mask.shape[0] == train_labels.shape[0]\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "\n",
        "# Create a DataLoader without the sampler using the oversampled dataset\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "\n",
        "# Define the learning rate scheduler using the oversampled dataset\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*num_epochs)\n",
        "\n",
        "# Training loop using the oversampled dataset\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
        "    for batch in progress_bar:\n",
        "        # Move data to GPU if available\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=batch[2])\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.logits, batch[2])\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update the progress bar\n",
        "        progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1)})\n",
        "\n",
        "    # Print loss for the epoch\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {total_loss / len(train_loader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbtP0rCksh1v",
        "outputId": "061d336f-12f6-4f93-d654-e55d6cc0186e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 | Loss: 0.6300374585570712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 | Loss: 0.6020110899751837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 | Loss: 0.5650223285862894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with class weights"
      ],
      "metadata": {
        "id": "-n-4ZT4-g2eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.utils.data import WeightedRandomSampler, DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming train_df is a DataFrame with columns 'tokenized_text' and 'Had Timing Objection'\n",
        "# 'tokenized_text' is expected to be a dictionary with keys 'input_ids' and 'attention_mask'\n",
        "\n",
        "# Prepare the data loaders\n",
        "train_input_ids = torch.cat(train_df['tokenized_text'].apply(lambda x: torch.tensor(x['input_ids'])).tolist(), dim=0)\n",
        "train_attention_mask = torch.cat(train_df['tokenized_text'].apply(lambda x: torch.tensor(x['attention_mask'])).tolist(), dim=0)\n",
        "train_labels = torch.tensor(train_df['Had Timing Objection'].values)\n",
        "\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "\n",
        "# Calculate class weights\n",
        "y_labels = train_df['Had Timing Objection'].values\n",
        "class_weights = compute_class_weight('balanced', classes=[0, 1], y=y_labels)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Ensure y_labels is a numpy array of consistent numeric type\n",
        "y_labels = np.asarray(y_labels, dtype=np.int64)\n",
        "\n",
        "# Calculate sample weights\n",
        "sample_weights = class_weights[y_labels]\n",
        "sample_weights = torch.tensor(sample_weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "# Create a WeightedRandomSampler\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Calculate the ratio of negative to positive samples for pos_weight\n",
        "pos_weight = class_weights[0] / class_weights[1]\n",
        "pos_weight_tensor = torch.tensor([pos_weight], dtype=torch.float).to(device)\n",
        "\n",
        "# Define a BCE loss with the calculated class weights\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
        "\n",
        "criterion.to(device)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*num_epochs)\n",
        "\n",
        "# Create a DataLoader with the sampler\n",
        "train_loader = DataLoader(train_data, batch_size=8, sampler=sampler)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
        "    for batch in progress_bar:\n",
        "        # Move data to GPU if available\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=batch[2].unsqueeze(1).float())\n",
        "\n",
        "        # Compute loss using class weights\n",
        "        loss = criterion(outputs.logits, batch[2].unsqueeze(1).float())\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update the progress bar\n",
        "        progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1)})\n",
        "\n",
        "    # Print loss for the epoch\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {total_loss / len(train_loader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3OnH-O1g3DR",
        "outputId": "3123cca2-3165-4083-f2cf-3d0c636c5695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-48d926398364>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_input_ids = torch.cat(train_df['tokenized_text'].apply(lambda x: torch.tensor(x['input_ids'])).tolist(), dim=0)\n",
            "<ipython-input-36-48d926398364>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_attention_mask = torch.cat(train_df['tokenized_text'].apply(lambda x: torch.tensor(x['attention_mask'])).tolist(), dim=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 | Loss: 0.3234663508832455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 | Loss: 0.28307280465960505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 | Loss: 0.25752780586481094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss decreases from epoch 1 to epoch 3, which is a good sign. However, there is a slight increase from epoch 1 to epoch 2. This could be due to the stochastic nature of gradient descent, especially since our dataset is not too large."
      ],
      "metadata": {
        "id": "-FECQUr3iaf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with oversampleing and class weights"
      ],
      "metadata": {
        "id": "Ns_6lBoXydEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data\n",
        "tokenized_texts = tokenizer(list(train_df_oversampled['Transcript']), truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "train_input_ids = tokenized_texts['input_ids']\n",
        "train_attention_mask = tokenized_texts['attention_mask']\n",
        "\n",
        "# Ensure that labels are a tensor and have the correct shape\n",
        "train_labels = torch.tensor(train_df_oversampled['Had Timing Objection'].values).unsqueeze(1)\n",
        "\n",
        "# Check that all tensors have the same number of rows\n",
        "assert train_input_ids.shape[0] == train_attention_mask.shape[0] == train_labels.shape[0]\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced', classes=[0, 1], y=train_df_oversampled['Had Timing Objection'].values)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Ensure that class labels are integers\n",
        "class_labels = train_df_oversampled['Had Timing Objection'].values.astype(int)\n",
        "\n",
        "# Handle any potential missing values in class_labels (if necessary)\n",
        "# For example, you could replace missing values with a default class label (e.g., 0)\n",
        "# class_labels = np.nan_to_num(class_labels, nan=0).astype(int)\n",
        "\n",
        "# Calculate sample weights\n",
        "sample_weights = np.array([class_weights[label] for label in class_labels])\n",
        "\n",
        "# Ensure sample_weights is of float type\n",
        "sample_weights = sample_weights.astype(np.float32)\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "sample_weights = torch.tensor(sample_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Create a WeightedRandomSampler\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Calculate the ratio of negative to positive samples for pos_weight\n",
        "pos_weight = class_weights[0] / class_weights[1]\n",
        "pos_weight_tensor = torch.tensor([pos_weight], dtype=torch.float).to(device)\n",
        "\n",
        "# Define a BCE loss with the calculated class weights\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
        "criterion.to(device)\n",
        "\n",
        "# Create a DataLoader with the sampler\n",
        "train_loader = DataLoader(train_data, batch_size=8, sampler=sampler)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*num_epochs)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
        "    for batch in progress_bar:\n",
        "        # Move data to GPU if available\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=batch[2].float())\n",
        "\n",
        "        # Compute loss using class weights\n",
        "        loss = criterion(outputs.logits, batch[2].float())\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update the progress bar\n",
        "        progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1)})\n",
        "\n",
        "    # Print loss for the epoch\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {total_loss / len(train_loader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFnsU9I1ygXl",
        "outputId": "b4a0b79b-eec7-4bb4-9737-30dc0716f254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 | Loss: 0.5724569577159304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 | Loss: 0.5503697603037863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 | Loss: 0.5454881100943594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming test_df['tokenized_text'] contains the output of the tokenizer\n",
        "test_input_ids = torch.cat(test_df['tokenized_text'].apply(lambda x: x['input_ids']).tolist(), dim=0)\n",
        "test_attention_mask = torch.cat(test_df['tokenized_text'].apply(lambda x: x['attention_mask']).tolist(), dim=0)\n",
        "test_labels = torch.tensor(test_df['Had Timing Objection'].values)\n",
        "\n",
        "# Prepare the test data loader\n",
        "test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
        "test_loader = DataLoader(test_data, batch_size=8)\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        # Move data to GPU if available\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=batch[0], attention_mask=batch[1])\n",
        "        preds = torch.sigmoid(outputs.logits).round().cpu().numpy()\n",
        "        labels = batch[2].cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.astype(int))  # Convert boolean to int here\n",
        "\n",
        "# Convert predictions to a flat list\n",
        "all_preds = [item for sublist in all_preds for item in sublist]\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(all_labels, all_preds, target_names=['Not Had Timing Objection', 'Had Timing Objection']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g5ASYAOg8Qi",
        "outputId": "17043e62-4115-433d-dbd4-d06668a96ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "Not Had Timing Objection       1.00      0.44      0.61        32\n",
            "    Had Timing Objection       0.31      1.00      0.47         8\n",
            "\n",
            "                accuracy                           0.55        40\n",
            "               macro avg       0.65      0.72      0.54        40\n",
            "            weighted avg       0.86      0.55      0.58        40\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4MS1Ekn6iWHs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}